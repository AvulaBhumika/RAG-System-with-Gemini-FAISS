{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c182ab-0c5e-456c-8c25-a315213b35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (0.8.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.11.0.post1-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-docx in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (2.176.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-generativeai) (4.13.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.52.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.32.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Downloading faiss_cpu-1.11.0.post1-cp312-cp312-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 4.7/14.9 MB 28.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.9/14.9 MB 26.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.7/14.9 MB 25.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 24.0 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2, faiss-cpu, sentence-transformers\n",
      "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.11.0.post1 sentence-transformers-5.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai faiss-cpu sentence-transformers PyPDF2 python-docx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03c5ece-fe2f-4c84-b82e-dfad9f78ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-kerasNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.6.15)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bhumi\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 18.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~andas (C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4e8d93-87dd-4d8b-8e31-976727be1a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656427556512408b89b0f4a8c2c21e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhumi\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bhumi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83cda6bf34c4f78a03ceddfe1b81456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df0b7c1d11744bfa90a66518b878137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9cf7fd25344914a0d5c855dc960eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad20b4d29404d678db6d2ad4b4a7d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ae19920d8b4d90abe4e703fa4c8053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9563ddceb19e41ebbbee7d31329c6b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f5e37346644cfc8acece156edaafb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2018419b56c7417f92a7abad26566d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d06cb6d77b04b93a458f0ad959d8c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5495d2ad811c42c9bfff4b1bcac8bbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 37 chunks from C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n",
      "Index saved to my_rag_index\n",
      "\n",
      "=== RAG System Ready ===\n",
      "Ask questions about your documents (type 'quit' to exit):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Explain Junk in, Junk out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Based on the context, \"Junk in, Junk out\" is a concept related to the overabundance and unmanageability of information, as indicated by the quote from Neil Postman: \"Information has become a form of garbage. We don’t know what to do with it, have no control over it; don’t know how to get rid of it.” It also references keeping \"the garbage out of your mind\" and not letting \"tiny things become toxic\".\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  Why\n",
      "\n",
      "Question:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: I am sorry, but the answer to the question cannot be found in the context.\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  DO NOT LET THE ‘TINY THINGS BECOME TOXIC’ \n",
      "\n",
      "Question:  DO NOT LET THE ‘TINY THINGS BECOME TOXIC’ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: 12 \n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  DO NOT LET THE ‘TINY THINGS BECOME TOXIC’ \n",
      "\n",
      "Question:  DO NOT LET THE ‘TINY THINGS BECOME TOXIC’ , explain this\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The provided text does not explicitly explain \"DO NOT LET THE 'TINY THINGS BECOME TOXIC'\". However, the surrounding content suggests it refers to the importance of managing one's thoughts and information intake, preventing negative or trivial matters from becoming overwhelming or detrimental. The phrase \"Junk in, Junk Out\" and the quote from Neil Postman about information overload further support this interpretation.\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  Summarise the concepts chapter wise\n",
      "\n",
      "Question:  Summarise the concepts chapter wise\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Here's a chapter-wise summary of the concepts discussed in the provided text:\n",
      "\n",
      "*   **Chapter 25:** Introduces the concept that \"Thought is the Mother of intention\".\n",
      "*   **Chapter 26:** Presents a \"FOUNDATIONAL PRINCIPLE\" related to a \"beautiful mind and a successful life\".\n",
      "*   **Chapter 27:** Continues exploring \"Foundational principles of a beautiful mind and a successful life.\"\n",
      "*   **Chapter 35:** Focuses on \"Stop Overthinking in just three minutes.\"\n",
      "*   **Chapter 36:** Highlights the idea that \"YOUR MIND IS YOUR LIFE MENTOR.\"\n",
      "*   **Chapter 37:** Further discusses \"Stop Overthinking in just three minutes.\"\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Documents:\n",
      "\n",
      "1. (Score: 0.210)\n",
      "Text: \u0003 \u0004\u0002 \u0004\u0014\u0013\u0013\u000f \u000f \u0006\u000e \u0004 \u0016\u0002\u0003\u0004\u0005 \u0005\u0004\u0005 \u0013\u0016\u0004 \u0006\u000f \u0004\u0018\u0014\u0011\u0004 D \u0013 \u0014 \u000f\u0002 \u000f\u0002\u0003\u000e \u0004\u0017 \u0014\u0003 \u0004>)\u000f\u0004 \u0014\u0013\u0016\u0004\u0005\u0002 \u0004 \u0006\u0004 \u0015\u0004 \u0006\u000f \u0004\u0003\u0006 \u0011@\u0004 Thought is the Mother of intention 25 \u0016 +=\u000f--\u0016,\u0011\u000f\u0010-\u0016\u0001\u0018\u0017\u0001\u0011\u000e\u0016\u0001!\u0014\u0017\u0011\u0001,\u0014#\u0016 +\u0012 \u0001#\u0016\u000f,\u0014\u0010\u0001...\n",
      "\n",
      "2. (Score: 0.207)\n",
      "Text: \u0004 \u0018 \u0002\u0006&\u0004 \u0006\u000f\u0002\u0003\u000e\u000f\u0004\u0007 \u0004\u0005 \u0003\u0004\u000f \u001a \u0011\u0004 1\u0002&\u0004 \u0010\u000f \u0006\u0004 \u0016\u0002\u0003\u0004 \u0014 \u0004 \u0014\u0004 \u000f\u0014\u0013\u0013 \u0006\u000e \u0004 \u0002 \u0004\u0017\u0014 % &\u0004 \u0002 \u0004 \u0013 \u0006\u000e\u0004\u0013\u0002\u0010&\u0004\u0014 %\u0004\u0016\u0002\u0003 \u0013 ;\u0004 6\u000e\u000f\u0011\u0001\u0018\u0017\u0001\u0011\u000e\u0018\u0017\u0001\u0017\u0018\u0011\u0012\u000f\u0011\u0018\u0014\u0010\u0001\u0011 &\u0018\u0010 \u0001\u0011\u0014\u0001\u0011\u0016\u000f-\u000e\u0001!\u0016?\u0001 6\u000e\u000f\u0011\u0001@\u0012\u000f \u0018\u0011\u0018\u0016\u0017\u0001\u0015\u0014\u0001)\u0001\u0010\u0016\u0016\u0015\u0001\u0011\u0014\u0001\u0018!(\u0018(\u0016\u0019\u0001\u0018\u0010\u0001...\n",
      "\n",
      "3. (Score: 0.195)\n",
      "Text: \u0002\u0003 E \u0004 % \u0006\u000e\u0011\u0004 \u0006\u000f \u0004\u0006 \u0004 \u000f\u0014 &\u0004 )* \u0001\u0017\u000e\u000f \u0016\u0001\u0011\u000e\u0016\u0001+\u0014\u0012\u0010\u0015\u000f\u0011\u0018\u0014\u0010\u000f \u0001 , \u0018\u0010-\u0018, \u0016\u0017\u0001#\u000e\u0018-\u000e\u0001+\u0014 !\u0001\u0011\u000e\u0016\u0001(\u000f\u0017\u0018\u0017\u0001\u0014+\u0001 \u0014\u0014\u0015\u0001\u0011\u000e\u0018\u0010\u001b\u0018\u0010 \u0001 \u000f\u0010\u0015\u0001\u000f\u0001 \u0016\u000f\u0011\u0001!\u0018\u0010\u0015\u0001\u000f\u0010\u0015\u0001 \u0018+\u0016\u0003 \u0004 - \u0004\u0016\u0002\u0003\u0004 \u0003 \u0002\u0003 \u0002\u0004%\u0006\u0002\u0010=\u0004 26 \u0001\u0002\u0003\u0004\u0005\u0006 FOUNDATIONAL PRINCIPLE...\n",
      "\n",
      "4. (Score: 0.193)\n",
      "Text: \u0013\u0013\u0004 \u0013\u0004 \u0014\u0013\u001a \u0004\u0014\u0006\u0005\u0004\u001a\u0002 \u0004 \u0006\u0004 \u0002 \u0002\u0013\u0004 \u0002 \u0004\u0016\u0002\u0003 \u000f\u0002\u0003\u000e \u0011\u0004 > \u0004\u0018 \u0014 \u0005\u0004 \u0002 \u0004AB\u0004\u0005\u0014\u0016 \u0004 \u0002 \u0006\u0003\u0002\u0003 \u0013\u0016&\u0004\u0016\u0002\u0003\u0004 \u000f\u0014\u0013\u0013\u0004 \u0002 \u001a\u0004\u0014\u0004\u0006 \u0010\u0004\u000f\u0014\u0017\u0011@\u0004\u0001\u0002\u0003 \u000f\u0002\u0003\u000e \u0004\u0010 \u0013\u0013\u0004\u0014\u0004 \u0003\u0017 \u0005 \u0006 \u000e\u0004 \u0014\u0006\u0005\u0004\u0002\u000f \u0006% \u0006\u000e\u0004\u0010 \u0013\u0013\u0004\u0002\u0018\u0011\u0004 \u0001\u0002\u0003\u0007\u0013\u0013\u0004\u0017 \u0004\u001a\u0002 \u0004\u001a \u0006\u0005 \u0003...\n",
      "\n",
      "5. (Score: 0.161)\n",
      "Text: \u000f\u0004\u0014\u0004\u001a \u00056\u0013 \u0004 4\u0004\u000e\u0014#\u0001 (\u000f \u000f\u0010-\u0016\u0015\u0001 &\u0014\u0012 \u0001 !\u0018\u0010\u0015\u0001 \u0018\u0017\u0001 \u000f\u0010\u0015\u0001 \u000e\u0014#\u0001 \u0016++\u0016-\u0011\u0018'\u0016 &\u0001&\u0014\u0012\u0001\u0012\u0017\u0016\u0001\u0018\u0011\u0019\u0001!\u000f\u001b\u0016\u0017\u0001\u000f \u0001\u0011\u000e\u0016\u0001\u0015\u0018++\u0016 \u0016\u0010-\u0016\u0001\u0011\u0014 \u0001 &\u0014\u0012 \u0001#\u0014 \u0015\u0003\u0001 -\u0004 \u0005 \u0018\u0004 \u0003\u0006\u0005\u0014\u0006\u0005 \u0006\u000e\u0004 \u0002 \u0004 \u0006 \u0004 \u0002 \u0004 \u0016\u0002\u0003 \u0004 \u001a \u0006\u0005\u0004\u0010 \u0013\u0013\u0004\u0002\u0018 \u0006\u0004\u0003\u0018\u0004\u0006 \u0010\u0004\u0005 \u001a \u0006 \u0002\u0006 \u0004 \u0002 \u0004\u0018 \u0002...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Summarise the concepts chapter wise from 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: I am sorry, but I cannot summarise the concepts chapter wise from 1 as the context does not contain chapter information.\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  n\n",
      "\n",
      "Question:  who is Dr. Saloni Singh\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Dr. Saloni Singh is the author of the book which provides proven ways to relieve anxiety, stress, confusion and tap into the power of a calm and clear mind.\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  n\n",
      "\n",
      "Question:  who is Neil Postman\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Neil Postman is mentioned in the context as the author of the quote: “Information has become a form of garbage. We don’t know what to do with it, have no control over it; don’t know how to get rid of it.”\n",
      "\n",
      "\n",
      "Sources: C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Show retrieved documents? (y/n):  n\n",
      "\n",
      "Question:  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import PyPDF2\n",
    "from docx import Document\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF and DOCX document processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_pdf(file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text_from_docx(file_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX file\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DOCX: {e}\")\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        # Clean the text\n",
    "        text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + chunk_size\n",
    "            \n",
    "            if end < len(text):\n",
    "                sentence_ends = ['.', '!', '?']\n",
    "                for i in range(end, max(start + chunk_size // 2, end - 100), -1):\n",
    "                    if text[i] in sentence_ends and i + 1 < len(text) and text[i + 1] == ' ':\n",
    "                        end = i + 1\n",
    "                        break\n",
    "            \n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            start = end - overlap\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Main RAG system class\"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_api_key: str, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize RAG system\n",
    "        \n",
    "        Args:\n",
    "            gemini_api_key: Google Gemini API key\n",
    "            embedding_model: Sentence transformer model for embeddings\n",
    "        \"\"\"\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        self.gemini_model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        \n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product for cosine similarity\n",
    "        \n",
    "        self.documents = []\n",
    "        self.metadata = []\n",
    "        \n",
    "        self.doc_processor = DocumentProcessor()\n",
    "        \n",
    "    def add_document(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a document to the knowledge base\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to PDF or DOCX file\n",
    "        \"\"\"\n",
    "        file_ext = file_path.lower().split('.')[-1]\n",
    "        \n",
    "        # Extract text based on file type\n",
    "        if file_ext == 'pdf':\n",
    "            text = self.doc_processor.extract_text_from_pdf(file_path)\n",
    "        elif file_ext == 'docx':\n",
    "            text = self.doc_processor.extract_text_from_docx(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
    "        \n",
    "        if not text.strip():\n",
    "            raise ValueError(f\"No text extracted from {file_path}\")\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = self.doc_processor.chunk_text(text)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeddings = self.embedding_model.encode(chunks)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Add to FAISS index\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        # Store documents and metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            self.documents.append(chunk)\n",
    "            self.metadata.append({\n",
    "                'source': file_path,\n",
    "                'chunk_id': len(self.documents),\n",
    "                'text': chunk\n",
    "            })\n",
    "        \n",
    "        print(f\"Added {len(chunks)} chunks from {file_path}\")\n",
    "    \n",
    "    def search_similar_documents(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar documents using vector similarity\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of similar documents to return\n",
    "            \n",
    "        Returns:\n",
    "            List of similar document chunks with metadata\n",
    "        \"\"\"\n",
    "        if self.index.ntotal == 0:\n",
    "            return []\n",
    "        \n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "        \n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append({\n",
    "                    'text': self.documents[idx],\n",
    "                    'score': float(score),\n",
    "                    'metadata': self.metadata[idx]\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer using Gemini with retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context_chunks: Retrieved relevant text chunks\n",
    "            \n",
    "        Returns:\n",
    "            Generated answer\n",
    "        \"\"\"\n",
    "        # Combine context chunks\n",
    "        context = \"\\n\\n\".join(context_chunks)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"\n",
    "Based on the following context, please answer the question. If the answer cannot be found in the context, please say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.gemini_model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main query method that retrieves relevant documents and generates answer\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing answer, sources, and retrieved documents\n",
    "        \"\"\"\n",
    "        # Retrieve similar documents\n",
    "        similar_docs = self.search_similar_documents(question, k)\n",
    "        \n",
    "        if not similar_docs:\n",
    "            return {\n",
    "                'answer': \"No relevant documents found in the knowledge base.\",\n",
    "                'sources': [],\n",
    "                'retrieved_docs': []\n",
    "            }\n",
    "        \n",
    "        # Extract text chunks for context\n",
    "        context_chunks = [doc['text'] for doc in similar_docs]\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(question, context_chunks)\n",
    "        \n",
    "        # Extract unique sources\n",
    "        sources = list(set([doc['metadata']['source'] for doc in similar_docs]))\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'retrieved_docs': similar_docs\n",
    "        }\n",
    "    \n",
    "    def save_index(self, save_path: str) -> None:\n",
    "        \"\"\"Save the FAISS index and metadata\"\"\"\n",
    "        # Save FAISS index\n",
    "        faiss.write_index(self.index, f\"{save_path}.faiss\")\n",
    "        \n",
    "        # Save documents and metadata\n",
    "        with open(f\"{save_path}_data.pkl\", 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'documents': self.documents,\n",
    "                'metadata': self.metadata\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Index saved to {save_path}\")\n",
    "    \n",
    "    def load_index(self, load_path: str) -> None:\n",
    "        \"\"\"Load a previously saved FAISS index and metadata\"\"\"\n",
    "        # Load FAISS index\n",
    "        self.index = faiss.read_index(f\"{load_path}.faiss\")\n",
    "        \n",
    "        with open(f\"{load_path}_data.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.documents = data['documents']\n",
    "            self.metadata = data['metadata']\n",
    "        \n",
    "        print(f\"Index loaded from {load_path}\")\n",
    "\n",
    "def main():\n",
    "    GEMINI_API_KEY = \"API-KEY\"  \n",
    "    rag = RAGSystem(GEMINI_API_KEY)\n",
    "    \n",
    "    try:\n",
    "        rag.add_document(r\"C:\\Users\\Bhumi\\Downloads\\How-to-stop-overthinking-Kindle-Book.pdf\")\n",
    "        # rag.add_document(\"document2.docx\")/\n",
    "        \n",
    "        rag.save_index(\"my_rag_index\")\n",
    "        \n",
    "        print(\"\\n=== RAG System Ready ===\")\n",
    "        print(\"Ask questions about your documents (type 'quit' to exit):\")\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nQuestion: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                break\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            # Get answer\n",
    "            result = rag.query(question)\n",
    "            \n",
    "            print(f\"\\nAnswer: {result['answer']}\")\n",
    "            print(f\"\\nSources: {', '.join(result['sources'])}\")\n",
    "            \n",
    "            # Optionally show retrieved documents\n",
    "            show_docs = input(\"\\nShow retrieved documents? (y/n): \").lower() == 'y'\n",
    "            if show_docs:\n",
    "                print(\"\\nRetrieved Documents:\")\n",
    "                for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "                    print(f\"\\n{i}. (Score: {doc['score']:.3f})\")\n",
    "                    print(f\"Text: {doc['text'][:200]}...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5021a4-358d-41df-b1ec-a96eedbd6c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01aa32-6f93-4e8a-8507-20e123d08004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
